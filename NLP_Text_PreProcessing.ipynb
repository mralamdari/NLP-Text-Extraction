{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Text-PreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Wq31EKDPV0f_",
        "TYPEaAgXV-EN",
        "0mOPQn0DWFdD"
      ],
      "authorship_tag": "ABX9TyOqCuGgAHXlpdh7or9JdHEx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/NLP-Text-Processing/blob/main/NLP_Text_PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GD6K9gKUjIOy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
        "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
      ],
      "metadata": {
        "id": "lUrdtzgOoeHL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = re.sub(r'\\d+', '', corpus.lower())"
      ],
      "metadata": {
        "id": "SGHk8_EVogwC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "y4HoUeAZ2Z9S",
        "outputId": "f0ee1e54-9901-41ca-819e-f78d53daaf0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = corpus.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "8UIoZi_H5mJh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.punctuation)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fXYooP_f53ZM",
        "outputId": "b98c7be1-57bf-4f18-8826-79890d40ea06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ' '.join([token for token in corpus.split()])"
      ],
      "metadata": {
        "id": "A_FGtycN54l0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rZdDWisl7RIx",
        "outputId": "69084362-b048-4435-adbb-df3916f2370d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "spacy_model = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJB40W7A7RUl",
        "outputId": "73674853-db97-41ac-e5af-e71e68bb015b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 19.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenize the Text"
      ],
      "metadata": {
        "id": "7Qe5dDXaVTOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop Words"
      ],
      "metadata": {
        "id": "Wq31EKDPV0f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Words with NLTK"
      ],
      "metadata": {
        "id": "TYPEaAgXV-EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "GslfQw877VxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ae66a9-cb32-4c2d-fc34-db06d38d882b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_nltk = set(nltk.corpus.stopwords.words('english'))\n",
        "len(stop_words_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tbqzRAmVf1j",
        "outputId": "41d27d9f-8ae9-4395-ccdd-c3ea919b8c9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus_nltk = nltk.tokenize.word_tokenize(corpus)\n",
        "print(len(tokenized_corpus_nltk))\n",
        "tokenized_corpus_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT9D0XmnVoxx",
        "outputId": "86c26a68-efb7-4537-eb90-79a5cef12d02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'to',\n",
              " 'finalize',\n",
              " 'the',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'which',\n",
              " 'will',\n",
              " 'be',\n",
              " 'used',\n",
              " 'for',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'soon',\n",
              " 'it',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'by',\n",
              " 'the',\n",
              " 'ending',\n",
              " 'of',\n",
              " 'this',\n",
              " 'month',\n",
              " 'but',\n",
              " 'will',\n",
              " 'it',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'has',\n",
              " 'been',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words_without_stopwords_nltk = [i for i in tokenized_corpus_nltk if i not in stop_words_nltk] \n",
        "print(len(tokenized_words_without_stopwords_nltk))\n",
        "tokenized_words_without_stopwords_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySc_d36KWchx",
        "outputId": "ee5ef288-d5ee-44cd-ddaf-c4486e0a197a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'finalize',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'used',\n",
              " 'notebook',\n",
              " 'done',\n",
              " 'soon',\n",
              " 'done',\n",
              " 'ending',\n",
              " 'month',\n",
              " 'notebook',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Words with Spacy"
      ],
      "metadata": {
        "id": "0mOPQn0DWFdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "stop_words_sparcy = spacy_model.Defaults.stop_words\n",
        "len(stop_words_sparcy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0v-e12CVpwt",
        "outputId": "5e22e68f-fbcc-491c-e617-cd606822f59a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus_sparcy = nltk.tokenize.word_tokenize(corpus)\n",
        "print(len(tokenized_corpus_sparcy))\n",
        "tokenized_corpus_sparcy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78E4HiAyXqK7",
        "outputId": "576691d6-3fa1-4493-d7d6-3591b18d7b37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'to',\n",
              " 'finalize',\n",
              " 'the',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'which',\n",
              " 'will',\n",
              " 'be',\n",
              " 'used',\n",
              " 'for',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'soon',\n",
              " 'it',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'by',\n",
              " 'the',\n",
              " 'ending',\n",
              " 'of',\n",
              " 'this',\n",
              " 'month',\n",
              " 'but',\n",
              " 'will',\n",
              " 'it',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'has',\n",
              " 'been',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words_without_stopwords_sparcy = [i for i in tokenized_corpus_sparcy if i not in stop_words_sparcy] \n",
        "print(len(tokenized_words_without_stopwords_sparcy))\n",
        "tokenized_words_without_stopwords_sparcy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg9MuBakXhgf",
        "outputId": "9f8fe122-70ea-4eb4-a530-135d2553f094"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'finalize',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'notebook',\n",
              " 'soon',\n",
              " 'ending',\n",
              " 'month',\n",
              " 'notebook',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The Difference between nltk and sparcy in stop words:\\n')\n",
        "set(tokenized_words_without_stopwords_nltk) - set(tokenized_words_without_stopwords_sparcy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBKvuJ6BXjGS",
        "outputId": "87a82244-ae86-43c0-9f7f-9ad48d037693"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Difference between nltk and sparcy in stop words:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'done', 'used'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyenchant\n",
        "import enchant\n",
        "from enchant import DictWithPWL\n",
        "from enchant.checker import SpellChecker\n",
        "\n",
        "my_dict = DictWithPWL(\"en_US\", \"mywords.txt\")\n",
        "my_checker = SpellChecker(my_dict)\n",
        "\n",
        "my_checker.set_text(\"This is sme sample txt with erors.\")\n",
        "for error in my_checker:\n",
        "    print (\"ERROR:\", error.word)"
      ],
      "metadata": {
        "id": "s663r3tuYbdz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "19345478-f237-4cae-87db-9d529f394c3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▉                          | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 20 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 30 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 40 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 51 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fc264338479c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyenchant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictWithPWL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/enchant/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_enchant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYENCHANT_IGNORE_MISSING_LIB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/enchant/_enchant.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: The 'enchant' C library was not found and maybe needs to be installed.\nSee  https://pyenchant.github.io/pyenchant/install.html\nfor details\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization"
      ],
      "metadata": {
        "id": "4n3WpiEo55c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "o0uYIS7D2ZFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tokenized_words_without_stopwords_nltk:\n",
        "  print(lemmatizer.lemmatize(word), end=' ')"
      ],
      "metadata": {
        "id": "wSrNiBXU6eFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tokenized_corpus_nltk:\n",
        "  print(lemmatizer.lemmatize(word), end=' ')"
      ],
      "metadata": {
        "id": "k0PWLK-564M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###POS Tagging"
      ],
      "metadata": {
        "id": "YPFHo1nE7y1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = spacy_model(corpus_original)"
      ],
      "metadata": {
        "id": "xsEjw8BO7mJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token, token.pos_, token.tag_)"
      ],
      "metadata": {
        "id": "TzwS1PGs8W_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Augmentation"
      ],
      "metadata": {
        "id": "8mJxfE2bA3CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug\n",
        "import nlpaug\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n"
      ],
      "metadata": {
        "id": "O0p0g8yK8kQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923433a2-6db2-485d-b508-5c6d729c1c08"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 61 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 92 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 102 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 112 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 122 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 133 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 143 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 163 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 174 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 184 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 194 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 204 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 215 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 225 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 235 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 245 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 256 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 266 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 276 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 286 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 296 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 307 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 317 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 327 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 337 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 348 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 358 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 368 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 378 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 389 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 399 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 410 kB 22.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (3.0.4)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_texts = nac.OcrAug().augment(corpus_original, n=20)\n",
        "augmented_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct4Z-Y27B1bU",
        "outputId": "ff6e5a92-6923-4deb-918c-f36fc1c4a08c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Need to finalize the demo corpus which will be o8ed for this notebook and it shuu1d be done s00n! !. It should 6e done 6y the ending uf thi8 month. But will it? This notebook has been run 4 times!!',\n",
              " 'Need to finalize the demo corpus which will be o8ed for this notebook and it should be done s00n! !. 1t should be done by the ending uf this month. But will it? Thi8 notebook has been run 4 times!!',\n",
              " 'Need tu fina1i2e the dem0 corpus which will be used for this notebook and it should 6e done soon! !. It should be done 6y the ending 0f this month. But will it? This notebook has been run 4 times!!',\n",
              " 'Need to finalize the demo corpus which wi11 be used for this notebook and it sh0old be done soon! !. 1t should be done by the ending 0f this munth. But will it? Thi8 notebook has been run 4 times!!',\n",
              " 'Need to finalize the demo corpus which will be used for this notebook and it should be dune soon! !. 1t should be done by the ending of this m0nth. But will it? This n0te6oor ha8 6een run 4 time8!!',\n",
              " 'Need t0 finalize the demo cokpos which will be o8ed fuk this notebook and it should be done soon! !. It should be dune by the endin9 of this month. But will it? This notebook has been kon 4 times!!',\n",
              " 'Need to fina1i2e the demu corpus which will be used for thi8 notebook and it should be done soon! !. It 8hoold be done by the endin9 of this month. But will it? This notebook has 6een run 1 times!!',\n",
              " 'Need to finalize the demo corpus which will be used fuk this notebook and it sh0u1d be done soon! !. It should be dune by the endin9 uf this month. But wi11 it? This notebook has been kon 1 times!!',\n",
              " 'Need to fina1i2e the demo corpus which will be used fuk this notebook and it should be done soon! !. It 8huuld be done by the ending of thi8 month. But wi11 it? This notebook ha8 6een kon 4 times!!',\n",
              " 'Need to finalize the demu corpus which will 6e used f0k this notebook and it should be dune soon! !. It shuu1d 6e dune by the ending of this month. But will it? This notebook has been run 4 times!!',\n",
              " 'Need to finalize the demo corpus which will 6e o8ed for this n0te6ouk and it should 6e done soon! !. It should be dune by the ending of this m0nth. But will it? This notebook has been run 4 times!!',\n",
              " 'Need to fina1i2e the demo corpus which will be used for this notebook and it should be done soon! !. It should be done by the endin9 of this month. But wi11 it? Thi8 notebook has been kon 4 times!!',\n",
              " 'Need to finalize the demo cokpos which wi11 be used fuk this notebook and it should be done soon! !. It should be d0ne by the ending uf thi8 month. Eot will it? This notebook ha8 been run 4 times!!',\n",
              " 'Need to fina1i2e the demo corpus which wi11 be used for this notebook and it should be done soon! !. It should be done by the endin9 of this month. But wi11 it? This n0te6oor ha8 6een run 4 times!!',\n",
              " 'Need to fina1i2e the demo corpus which will be o8ed for thi8 notebook and it sh0u1d be done soon! !. It should be done by the ending of this month. Eot wi11 it? This notebook has been run 4 times!!',\n",
              " 'Need to finalize the demu corpus which wi11 be used for this notebook and it should be d0ne soon! !. 1t should be dune by the ending of this month. But will it? This n0tebour has been run 4 times!!',\n",
              " 'Need to fina1i2e the demo corpo8 which will be used for thi8 notebook and it should be done 8o0n! !. It should 6e d0ne by the ending of thi8 month. But will it? This notebook has been run 4 times!!',\n",
              " 'Need to finalize the demo curpu8 which will 6e used f0k this notebook and it should be done soon! !. 1t should 6e dune by the ending of this month. But will it? This notebook has 6een run 1 times!!',\n",
              " 'Need to finalize the demo corpus which wi11 be used for this notebook and it should be done soon! !. It shoo1d 6e d0ne 6y the endin9 of this month. Eot wi11 it? This notebook ha8 been run 1 times!!',\n",
              " 'Need to finalize the demo curpos which will be used for thi8 notebook and it should be done soon! !. It 8hoold be done by the ending of this month. But will it? This notebook has been kon 4 time8!!']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_texts = nac.KeyboardAug().augment(corpus_original, n=20)\n",
        "augmented_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bur_r0LC__5",
        "outputId": "c333596a-b876-4924-ea9c-0e8aa4216040"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N3eF to f8nSlise the dFko corpus which will be used for ^hiE notebook and it shkulR be done Eo9n! !. It wh8uld be done by the ending of tM9s month. But djll it? This notebook has veeB run 4 times!!',\n",
              " 'Need to fiJal7Ae the demo corpus which will be used for this no$2b(ok and it sh)uKd be XLne soon! !. It shKulV be EKne by the ending of this month. But wKlI it? hbis noFeboPL has been run 4 6umes!!',\n",
              " 'Need to finalize the eeHo cogous #h*ch w8Il be uAwd for rhLs notebook and it should be dohD Aion! !. It should be done by the end*gg of this month. But will it? This noHebIKk has been run 4 times!!',\n",
              " 'Need to finalize the C@mo corp8x wUicg wi:< be used for this notebook and it Dh0uld be done c)on! !. It shluOd be Con@ by the ending of thkq month. But will it? This notebook has bfeH run 4 times!!',\n",
              " 'Need to fibSliz4 the vdmo cor9*s whixn will be usFv for this notsfool and it should be done so)j! !. It should be done by the ending of this month. But woil it? This nofefoPk has been run 4 tLmeD!!',\n",
              " 'Mewd to finQliaW the vemk corpus shicu will be used for thjd m8Yebook and it Ahoukd be dKnS so(H! !. It should be done by the ending of this month. But 1il> it? This notebook has been run 4 times!!',\n",
              " 'Need to finalize the dRno corpus Ehlch will be 6seE for thkd notwb8lk and it should be voJe Qo0n! !. It should be done by the FndinV of ^hiw month. But #iol it? This notebook has been run 4 times!!',\n",
              " 'NeSs to f8Jalkze the demo corpus which wi>> be uZ2d for this not4Foo. and it shoH,d be done soon! !. It should be done by the ending of this kontM. But will it? Th9W notebook has beRh run 4 4ines!!',\n",
              " 'N4dd to fJnaPizR the demo Forp6s wNicj will be us#f for this notebook and it Zhpuld be done s(0n! !. It should be done by the ending of HhOs month. But wJli it? This noYegoPk has been run 4 times!!',\n",
              " 'Nerc to finalize the ffmo Vorpud which will be js$d for tJos notebook and it WhouOd be done soon! !. It should be done by the enfinN of Gjis month. But wllO it? This notebook has heeH run 4 times!!',\n",
              " 'NDeF to d(jalize the demo corpus which will be used for this notebook and it xh8uld be Won@ s9oJ! !. It eh)uld be conf by the ending of this month. But aJll it? yhLs noG$blok has been run 4 times!!',\n",
              " 'Need to finalize the dDmK corp tz which will be used for this MotebIik and it should be done skoM! !. It should be dkHe by the ending of Htis month. But will it? TGiW MotDboom has b#Rn run 4 tim#c!!',\n",
              " 'beeV to finalize the dekI corpus which wioo be used for this notebook and it shLuls be done soon! !. It should be done by the endony of RhiE month. But !Lll it? $nis notebook has b@#n run 4 $iNes!!',\n",
              " 'Jsed to finalize the dfJo corpus which will be used for 6Tis notebook and it should be d0Be so0G! !. It should be dojd by the dndijg of this month. But !ilP it? 4h*s notebook has hefn run 4 times!!',\n",
              " 'Need to finalize the demo corpus which wul< be uXeW for this notebook and it ehoild be W0ne Cion! !. It should be conD by the ending of this month. But EUll it? Tm8s notebook has bweB run 4 t(Nes!!',\n",
              " 'Ge4d to finalize the demo coG(us which will be 6s2d for this noFSbooU and it shou;v be done soon! !. It should be done by the ending of tTUs mPngh. But will it? YMis notebook has be2j run 4 t8m$s!!',\n",
              " 'Need to finalize the demo ckrOus which will be used for this notebook and it ChoulF be e8ne s*In! !. It should be doB@ by the eJdinT of this month. But wolO it? 4h7s notFG8ok has bRWn run 4 times!!',\n",
              " 'Need to fkgalizf the femI coDpuE whiXY wi// be used for this notebook and it sjoulC be done qo(n! !. It should be done by the ending of this konYh. But will it? This notebook has bRrn run 4 tO<es!!',\n",
              " 'Need to finalize the F3mo corpus ehivh will be JCed for 4Yis notebook and it suoulx be come soon! !. It shou>V be done by the ending of this KontM. But will it? ThJC noteFoli has been run 4 times!!',\n",
              " 'NFsd to ViMa;ize the demo So4pus wh7Sh will be uX3d for this notebook and it should be dojW qion! !. It should be done by the Dbding of this noBth. But will it? This notebook has ve$n run 4 times!!']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_texts = naw.SpellingAug().augment(corpus_original, n=20)\n",
        "augmented_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWp9OxOHDm2_",
        "outputId": "25101613-78dc-4cb1-e6e9-b1b61167e11b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Need yto finalize the demo corpus whitch wiil be used for thies notebook ahd IT should be dane soonly! !. It shuld be done by the ending of ths month. But will it? This notebook has been run 4 times!!',\n",
              " 'Need to finalize the demo corpus which wll bem used fore this notebook and it should be down soon! !. It should he doen by thhe ending ow this Mounth. But will it? This notebook has been rum 4 times!!',\n",
              " 'Need to finalize thetwo demo corpus which will be use for this notebook en it should me done soom! !. It should b done by the ending of this month. But will it? This notebook ha been rung 5 teams!!',\n",
              " \"Need tp finalize thes demo corpus whice will be used for this notebook cndy it should be done soon! !. It''s shouid be done Buy the endding of this month. But will lt? This netbook has been run 4 times!!\",\n",
              " 'Need trto finalize the demo corpus which wiil be used ffor this notbook and it should be done soon! !. It should br down by the ending oft this month. But will it? Thins notebook as been rug 4 times!!',\n",
              " 'Need to finalize the demo corpus which woll be usee for yhis notebook and it should me done so! !. It should bem done Buy the ending of this month. But wall it? This notebook has been running 4 timens!!',\n",
              " 'Need t0 finalize the demo corpus which will be used for thease notebook and it should be doen soon! !. It shoule be done by the ending ow this month. But wiIl il? Thi notebook hav been run 4 time!!',\n",
              " \"Need to finalize the demo corpus whice will be usd gor this nootebook and it's. should be done soon! !. It should me done by the ending jf htis month. But will it? This netbook has been runin 4 times!!\",\n",
              " \"Need to finalize zhe demo corpus which w'll me used for this notebook and is should be done soon! !. It should we done by the ending oh ti omonth. But will is? This notebook has been run 4th times!!\",\n",
              " 'Need lo finalize the demo corpus which will be used for this notebook and it should be done soon! !. lt shoull by done by thee ending ol this month. But wild itv? This notebook hes beeb run 4 times!!',\n",
              " \"Need to finalize the demo corpus whuch wll be used for this notebook and iw should be dane soon! !. It should be done by the endding of thies monthe. But will it? Thia's nootebook hav been run 4 times!!\",\n",
              " 'Need to finalize the demo corpus which will br used for this notebook and it should br down soon! !. /It should be doen by tht ending of ths month. But will it? This netbook has been rune 4 time!!',\n",
              " 'Need to finalize the demo corpus which will be used for theis notebook and it shouls be down soon! !. lt should se dane Bye the ending of this month. But will it? This notebook haves been ran 5 times!!',\n",
              " 'Need t finalize the demo corpus which will bem used for thise notebook and it shleould be done soon! !. It shoult be done yb the end aof this monteh. Buth will it? This notebook has been run 4 times!!',\n",
              " \"Need take finalize the demo corpus whic will bem used for this notebook and in should be doen sun! !. It should be done by the end of this month. But w'll it? This notebook has bee run four times!!\",\n",
              " 'Need to finalize the demo corpus which wild be used for this nootebook and it should b doen soon! !. I shouold be done xby the ending of this month. But wil iy? This notebook has being run 4 times!!',\n",
              " 'Need to finalize the demo corpus hich will be used for thous notebook and it shloud be down soon! !. IT should be done by that ending jf this month. But wiIl it? These notebook has beans run 4 times!!',\n",
              " \"Need to finalize the demo corpus which will me used for ti notbook and it shoudt be dane soon! !. It's should be doen bu the ending of this month. But will ist? Thys notebook has been run 4 times!!\",\n",
              " 'Need to finalize the demo corpus wihch will be used dor yhis notebook and it should me done oon! !. It chould be done by the ending of thid menth. But will it? thes notebook has been run 4 timens!!',\n",
              " 'Need to finalize the demo corpus which will be used por this nootebook and ii should [[bi dane soon! !. It sholud be done by the ending of this mont. But wil it? thes notebook has been ren 4 times!!']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 1"
      ],
      "metadata": {
        "id": "ezjxjJ4juRXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = '''\n",
        "man bite dog\n",
        "dog bite man\n",
        "man eat meat\n",
        "dog eat meat\n",
        "man like ball\n",
        "man like dog\n",
        "'''"
      ],
      "metadata": {
        "id": "4YjhFUFjEZWC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(sample_text.split())"
      ],
      "metadata": {
        "id": "Du6h6gHUvCWH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RggdJQ7pEJP0",
        "outputId": "2c795b2b-f87a-4224-98b6-285eaa6f856c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ball', 'bite', 'dog', 'eat', 'like', 'man', 'meat'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FJ5w5urpELYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}