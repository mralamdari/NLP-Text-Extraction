{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Text-PreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Wq31EKDPV0f_",
        "TYPEaAgXV-EN",
        "0mOPQn0DWFdD"
      ],
      "authorship_tag": "ABX9TyMQb/IIeoKpRC4WypNew3LA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/NLP-Text-Processing/blob/main/NLP_Text_PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GD6K9gKUjIOy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
        "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
      ],
      "metadata": {
        "id": "lUrdtzgOoeHL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = re.sub(r'\\d+', '', corpus.lower())"
      ],
      "metadata": {
        "id": "SGHk8_EVogwC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "y4HoUeAZ2Z9S",
        "outputId": "f0ee1e54-9901-41ca-819e-f78d53daaf0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = corpus.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "8UIoZi_H5mJh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.punctuation)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fXYooP_f53ZM",
        "outputId": "b98c7be1-57bf-4f18-8826-79890d40ea06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ' '.join([token for token in corpus.split()])"
      ],
      "metadata": {
        "id": "A_FGtycN54l0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rZdDWisl7RIx",
        "outputId": "69084362-b048-4435-adbb-df3916f2370d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "spacy_model = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJB40W7A7RUl",
        "outputId": "73674853-db97-41ac-e5af-e71e68bb015b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 19.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenize the Text"
      ],
      "metadata": {
        "id": "7Qe5dDXaVTOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop Words"
      ],
      "metadata": {
        "id": "Wq31EKDPV0f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Words with NLTK"
      ],
      "metadata": {
        "id": "TYPEaAgXV-EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "GslfQw877VxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ae66a9-cb32-4c2d-fc34-db06d38d882b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_nltk = set(nltk.corpus.stopwords.words('english'))\n",
        "len(stop_words_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tbqzRAmVf1j",
        "outputId": "41d27d9f-8ae9-4395-ccdd-c3ea919b8c9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus_nltk = nltk.tokenize.word_tokenize(corpus)\n",
        "print(len(tokenized_corpus_nltk))\n",
        "tokenized_corpus_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT9D0XmnVoxx",
        "outputId": "86c26a68-efb7-4537-eb90-79a5cef12d02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'to',\n",
              " 'finalize',\n",
              " 'the',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'which',\n",
              " 'will',\n",
              " 'be',\n",
              " 'used',\n",
              " 'for',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'soon',\n",
              " 'it',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'by',\n",
              " 'the',\n",
              " 'ending',\n",
              " 'of',\n",
              " 'this',\n",
              " 'month',\n",
              " 'but',\n",
              " 'will',\n",
              " 'it',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'has',\n",
              " 'been',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words_without_stopwords_nltk = [i for i in tokenized_corpus_nltk if i not in stop_words_nltk] \n",
        "print(len(tokenized_words_without_stopwords_nltk))\n",
        "tokenized_words_without_stopwords_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySc_d36KWchx",
        "outputId": "ee5ef288-d5ee-44cd-ddaf-c4486e0a197a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'finalize',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'used',\n",
              " 'notebook',\n",
              " 'done',\n",
              " 'soon',\n",
              " 'done',\n",
              " 'ending',\n",
              " 'month',\n",
              " 'notebook',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Words with Spacy"
      ],
      "metadata": {
        "id": "0mOPQn0DWFdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "stop_words_sparcy = spacy_model.Defaults.stop_words\n",
        "len(stop_words_sparcy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0v-e12CVpwt",
        "outputId": "5e22e68f-fbcc-491c-e617-cd606822f59a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus_sparcy = nltk.tokenize.word_tokenize(corpus)\n",
        "print(len(tokenized_corpus_sparcy))\n",
        "tokenized_corpus_sparcy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78E4HiAyXqK7",
        "outputId": "576691d6-3fa1-4493-d7d6-3591b18d7b37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'to',\n",
              " 'finalize',\n",
              " 'the',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'which',\n",
              " 'will',\n",
              " 'be',\n",
              " 'used',\n",
              " 'for',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'soon',\n",
              " 'it',\n",
              " 'should',\n",
              " 'be',\n",
              " 'done',\n",
              " 'by',\n",
              " 'the',\n",
              " 'ending',\n",
              " 'of',\n",
              " 'this',\n",
              " 'month',\n",
              " 'but',\n",
              " 'will',\n",
              " 'it',\n",
              " 'this',\n",
              " 'notebook',\n",
              " 'has',\n",
              " 'been',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words_without_stopwords_sparcy = [i for i in tokenized_corpus_sparcy if i not in stop_words_sparcy] \n",
        "print(len(tokenized_words_without_stopwords_sparcy))\n",
        "tokenized_words_without_stopwords_sparcy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg9MuBakXhgf",
        "outputId": "9f8fe122-70ea-4eb4-a530-135d2553f094"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'finalize',\n",
              " 'demo',\n",
              " 'corpus',\n",
              " 'notebook',\n",
              " 'soon',\n",
              " 'ending',\n",
              " 'month',\n",
              " 'notebook',\n",
              " 'run',\n",
              " 'times']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The Difference between nltk and sparcy in stop words:\\n')\n",
        "set(tokenized_words_without_stopwords_nltk) - set(tokenized_words_without_stopwords_sparcy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBKvuJ6BXjGS",
        "outputId": "87a82244-ae86-43c0-9f7f-9ad48d037693"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Difference between nltk and sparcy in stop words:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'done', 'used'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyenchant\n",
        "import enchant\n",
        "from enchant import DictWithPWL\n",
        "from enchant.checker import SpellChecker\n",
        "\n",
        "my_dict = DictWithPWL(\"en_US\", \"mywords.txt\")\n",
        "my_checker = SpellChecker(my_dict)\n",
        "\n",
        "my_checker.set_text(\"This is sme sample txt with erors.\")\n",
        "for error in my_checker:\n",
        "    print (\"ERROR:\", error.word)"
      ],
      "metadata": {
        "id": "s663r3tuYbdz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "19345478-f237-4cae-87db-9d529f394c3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▉                          | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 20 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 30 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 40 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 51 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fc264338479c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyenchant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictWithPWL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/enchant/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_enchant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYENCHANT_IGNORE_MISSING_LIB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/enchant/_enchant.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: The 'enchant' C library was not found and maybe needs to be installed.\nSee  https://pyenchant.github.io/pyenchant/install.html\nfor details\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization"
      ],
      "metadata": {
        "id": "4n3WpiEo55c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "o0uYIS7D2ZFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tokenized_words_without_stopwords_nltk:\n",
        "  print(lemmatizer.lemmatize(word), end=' ')"
      ],
      "metadata": {
        "id": "wSrNiBXU6eFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tokenized_corpus_nltk:\n",
        "  print(lemmatizer.lemmatize(word), end=' ')"
      ],
      "metadata": {
        "id": "k0PWLK-564M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###POS Tagging"
      ],
      "metadata": {
        "id": "YPFHo1nE7y1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = spacy_model(corpus_original)"
      ],
      "metadata": {
        "id": "xsEjw8BO7mJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token, token.pos_, token.tag_)"
      ],
      "metadata": {
        "id": "TzwS1PGs8W_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Augmentation"
      ],
      "metadata": {
        "id": "8mJxfE2bA3CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nlpaug\n",
        "import nlpaug\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n"
      ],
      "metadata": {
        "id": "O0p0g8yK8kQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_texts = nac.OcrAug().augment(corpus_original, n=20)\n",
        "augmented_texts"
      ],
      "metadata": {
        "id": "ct4Z-Y27B1bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_texts = nac.KeyboardAug().augment(corpus_original, n=20)\n",
        "augmented_texts"
      ],
      "metadata": {
        "id": "3Bur_r0LC__5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_texts = naw.SpellingAug().augment(corpus_original, n=20)\n",
        "augmented_texts"
      ],
      "metadata": {
        "id": "mWp9OxOHDm2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary"
      ],
      "metadata": {
        "id": "ezjxjJ4juRXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = '''\n",
        "man bite dog\n",
        "dog bite man\n",
        "man eat meat\n",
        "dog eat meat\n",
        "man like ball\n",
        "man like dog\n",
        "'''"
      ],
      "metadata": {
        "id": "4YjhFUFjEZWC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Du6h6gHUvCWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}