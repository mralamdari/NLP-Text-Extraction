{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Question-Duplicates-Beginner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLQpv7LyH4mL7wgSQ+pTUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-alamdari/NLP-Question-Duplicates-Beginner/blob/main/NLP_Question_Duplicates_Beginner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3dsb-v0XQSp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trax\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from trax.fastmath import numpy as fastnp"
      ],
      "metadata": {
        "id": "mMklRLPqXbPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef201287-da9f-4eae-90cd-b3d595f397c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trax\n",
            "  Downloading trax-1.4.1-py2.py3-none-any.whl (637 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 81 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 245 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 276 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 307 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 327 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 368 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 389 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 409 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 450 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 471 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 481 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 491 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 501 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 522 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 532 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 552 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 563 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 573 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 583 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 593 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 604 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 614 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 634 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 637 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.5.0)\n",
            "Collecting funcsigs\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.0)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.4)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->trax) (0.16.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax->trax) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.4.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.64.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.17.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.7)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (21.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->trax) (3.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.56.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.14.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (13.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.44.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 31.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (4.11.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->trax) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-text, funcsigs, trax\n",
            "Successfully installed funcsigs-1.0.2 tensorflow-text-2.8.2 tf-estimator-nightly-2.8.0.dev2021122109 trax-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
        "    input1 = []\n",
        "    input2 = []\n",
        "    idx = 0\n",
        "    len_q = len(Q1)\n",
        "    question_indexes = [*range(len_q)]\n",
        "    \n",
        "    if shuffle:\n",
        "        rnd.shuffle(question_indexes)\n",
        "    \n",
        "    while True:\n",
        "        if idx >= len_q:\n",
        "            idx = len_q\n",
        "            if shuffle:\n",
        "                rnd.shuffle(question_indexes)\n",
        "        \n",
        "        q1 = Q1[question_indexes[idx]]\n",
        "        q2 = Q2[question_indexes[idx]]\n",
        "        idx += 1\n",
        "        input1.append(q1)\n",
        "        input2.append(q2)\n",
        "        if len(input1) == batch_size:\n",
        "            max_len = max(max([len(q) for q in input1]),max([len(q) for q in input2]))\n",
        "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
        "            b1 = []\n",
        "            b2 = []\n",
        "            for q1, q2 in zip(input1, input2):\n",
        "                q1 = q1 + [pad] * (max_len - len(q1))\n",
        "                q2 = q2 + [pad] * (max_len - len(q2))\n",
        "                b1.append(q1)\n",
        "                b2.append(q2)\n",
        "            yield np.array(b1), np.array(b2)\n",
        "            input1, input2 = [], []\n",
        "            "
      ],
      "metadata": {
        "id": "rj1mj06taBZt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))"
      ],
      "metadata": {
        "id": "h8tP39aKkhqH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = lambda x: x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))"
      ],
      "metadata": {
        "id": "2sFSOGWvk2_F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Siamese(vocab_size, d_model=128, mode='train'):\n",
        "    q_processor = tl.Serial(\n",
        "        tl.Embedding(vocab_size, d_model),\n",
        "        tl.LSTM(d_model),\n",
        "        tl.Mean(axis=1),\n",
        "        tl.Fn('Normalize', lambda x: normalize(x)) \n",
        "    ) \n",
        "    \n",
        "    model = tl.Parallel(q_processor, q_processor)\n",
        "    return model"
      ],
      "metadata": {
        "id": "B9TUn91_km_L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TripletLossFn(v1, v2, margin=0.25):\n",
        "    scores = fastnp.dot(v1, v2.T)\n",
        "    batch_size = len(scores)\n",
        "    positive = fastnp.diagonal(scores)\n",
        "    negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
        "    closest_negative = negative_without_positive.max(axis=1)\n",
        "    negative_zero_on_duplicate = scores * (1.0 - fastnp.eye(batch_size))\n",
        "    mean_negative = np.sum(negative_zero_on_duplicate, axis=1) / (batch_size-1)\n",
        "    triplet_loss1 = fastnp.maximum(0.0, margin - positive + closest_negative)\n",
        "    triplet_loss2 = fastnp.maximum(0.0, margin - positive + mean_negative)\n",
        "    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
        "    return triplet_loss"
      ],
      "metadata": {
        "id": "07MtJdfvk-DY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
        "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
        "TripletLossFn(v2,v1)\n",
        "print(\"Triplet Loss:\", TripletLossFn(v2,v1))"
      ],
      "metadata": {
        "id": "LfIUw1ojlfED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "def TripletLoss(margin=0.25):\n",
        "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
        "    return tl.Fn('TripletLoss', triplet_loss_fn)"
      ],
      "metadata": {
        "id": "sEIQON8vlqk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab['<PAD>'])\n",
        "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab['<PAD>'])\n",
        "print('train_Q1.shape ', train_Q1.shape)\n",
        "print('val_Q1.shape   ', val_Q1.shape)"
      ],
      "metadata": {
        "id": "aYBHbhNxlvJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
        "\n",
        "def train_model(Siamese, TripletLoss, lr_schedule, train_generator, val_generator, output_dir='model/'):\n",
        "    output_dir = os.path.expanduser(output_dir)\n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_generator,            \n",
        "        loss_layer=TripletLoss(),                \n",
        "        optimizer=trax.optimizers.Adam(0.01),    \n",
        "        lr_schedule=lr_schedule,                  \n",
        "    )\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=val_generator,       \n",
        "        metrics=[TripletLoss()],          \n",
        "    )\n",
        "    training_loop = training.Loop(Siamese(),train_task,eval_task=eval_task,output_dir=output_dir)\n",
        "\n",
        "    return training_loop"
      ],
      "metadata": {
        "id": "81bKjlWQlyqf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = 100\n",
        "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
        "training_loop.run(train_steps)"
      ],
      "metadata": {
        "id": "3ZmE3mtPl6EG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Siamese()\n",
        "model.init_from_file('model.pkl.gz')"
      ],
      "metadata": {
        "id": "qztvWms6mbaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
        "    accuracy = 0\n",
        "    for i in range(0, len(test_Q1), batch_size):\n",
        "        q1, q2 = next(data_generator(test_Q1[i:i + batch_size], test_Q2[i:i + batch_size], batch_size, vocab['<PAD>'], shuffle=False))\n",
        "        y_test = y[i:i + batch_size]\n",
        "        v1, v2 = model((q1, q2))\n",
        "        for j in range(batch_size):\n",
        "            d = np.dot(v1[j], v2[j].T)\n",
        "            res = d > threshold\n",
        "            accuracy += (y_test[j] == res)\n",
        "    accuracy = accuracy / len(test_Q1)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "y23tajA8miWF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = classify(Q1_test,Q2_test, y_test, 0.7, model, vocab, batch_size = 512) "
      ],
      "metadata": {
        "id": "xVdEeSvDmiRw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
        "    q1 = nltk.word_tokenize(question1)  \n",
        "    q2 = nltk.word_tokenize(question2)  \n",
        "    Q1 = [vocab[word] for word in q1]\n",
        "    Q2 = [vocab[word] for word in q2]\n",
        "        \n",
        "    Q1, Q2 = next(data_generator([Q1], [Q2], 1, vocab['<PAD>']))\n",
        "    v1, v2 = model((Q1, Q2))\n",
        "    d = np.dot(v1[0], v2[0].T)\n",
        "    res = d > threshold\n",
        "    if(verbose):\n",
        "        print(\"Q1  = \", Q1, \"\\nQ2  = \", Q2)\n",
        "        print(\"d   = \", d)\n",
        "        print(\"res = \", res)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "fR0iqPSnmh4d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"When will I see you?\"\n",
        "question2 = \"When can I see you again?\"\n",
        "predict(question1 , question2, 0.7, model, vocab, verbose = True)"
      ],
      "metadata": {
        "id": "z5TpT_sonL4E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}